# LLMs from Scratch 🚀

Build and understand Large Language Models by implementing them from the ground up. This repository contains clean, educational implementations of popular transformer architectures with detailed explanations and runnable code.

## 🎯 What's Inside

| Model | Description |
|-------|-------------|
| **GPT-2** | Decoder-only transformer with causal attention | 
| **GPT-1** | Original GPT architecture | - |
| **BERT** | Encoder-only transformer | - |
| **T5** | Encoder-decoder transformer | - |
| **LLaMA** | Modern decoder with RMSNorm | - |


## 🏗️ Architecture Deep Dives

Each implementation includes:

- **📚 Theory**: Mathematical foundations and intuitive explanations
- **🔧 Code**: Clean, documented PyTorch implementations  
- **🧪 Experiments**: Training loops, text generation, and benchmarks
- **📊 Analysis**: Performance metrics and memory usage
- **🎮 Interactive**: Jupyter notebooks with runnable examples

## 🙏 Acknowledgments

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - The original Transformer paper
- [GPT-2 Paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) - OpenAI's GPT-2 
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) - Excellent visual explanations
- [nanoGPT](https://github.com/karpathy/nanoGPT) - Andrej Karpathy's minimal GPT implementation
- [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html) - Harvard NLP's line-by-line implementation
- [Transformers from Scratch](https://e2eml.school/transformers.html) - Brandon Rohrer's tutorial
- [Jay Alammar's Blog](https://jalammar.github.io/) - Outstanding transformer visualizations
- [Hugging Face](https://huggingface.co/) - For reference implementations and datasets
